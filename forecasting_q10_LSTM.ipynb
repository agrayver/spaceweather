{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Zonal Harmonic Forecasting via LSTM\n",
    "\n",
    "In this notebook, I attempt the most basic prediction for the first zonal harmonic of the external field in response to solar activity. The zonal harmonic, corresponding to the associated Legendre polynomial $P_1^0$, is effectively what is measured by the disturbance storm time (Dst) index, except the data that I am trying to reconstruct record only the external field response, while Dst inadvertently includes the fields generated in Earth's subsurface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import datetime as dt\n",
    "from helper_functions import *\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "from aux_keras import *\n",
    "\n",
    "# file management, io\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Let's load the data without gaps as well as the external coefficients. We'll use the unfiltered coefficients since the filtered coefficients eliminate quite a bit of small scale structure in the time series; in particular many jumps and abrupt changes in the external magnetic field are excessively smoothed and would cause the network to learn unrealistic behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni = pd.read_hdf('omni_hourly_1998-2018_nogaps.h5')\n",
    "coeff = pd.read_hdf('external_coefficients_unfiltered.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The external coefficients report hourly values centered on every half hour, starting at 0:30 on January 1, 1999 (UT). The OMNI series, however, are centered on the hour beginning at 0:00 on January 1, 1998, but contain data averaged throughout the subsequent hour (i.e. hour 0 contains data averaged from 0:00-1:00). While the documentation for the OMNI data do not mention this anywhere, they also report the times with respect to universal time UT, so the time formats are the same between the datasets. \n",
    "\n",
    "Thus, both datasets contain data averaged within the same time bins, the bin centers are just denoted differently with the centers at the beginning of the hour for OMNI and at the middle of the hour for the external coefficient series. \n",
    "\n",
    "The OMNI data go until 23:00 December 31, 2018, and the corresponding total number of expected data points is computed below and compared to the observed total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs_yr = 365*24\n",
    "hrs_yr_lp = 366*24\n",
    "\n",
    "n_yr = 21\n",
    "# 2000, 2004, 2008, 2012, 2016\n",
    "n_lp_yr = 5\n",
    "\n",
    "hrs_OMNI = hrs_yr*(n_yr-n_lp_yr) + hrs_yr_lp*n_lp_yr\n",
    "\n",
    "print('Expected number of hours in OMNI data set: %d' % hrs_OMNI)\n",
    "print('Number of hours in OMNI data set: %d' % len(omni))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above code computes, the anticipated number of hours in the OMNI data set is 184080, which is precisely what we get. \n",
    "\n",
    "Below, we perform a similar computation for the external coefficients, which span 0:30 January 1, 1999 to 22:30 December 31, 2018. Thus, there should be exactly one year and one hour fewer data points than the OMNI set, since the external coefficients end an hour early in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs_offset = hrs_yr + 1\n",
    "hrs_ext_coeff = hrs_OMNI - hrs_offset\n",
    "\n",
    "print('Expected number of hours in external coefficients data set: %d' % hrs_ext_coeff)\n",
    "print('Number of hours in external coefficients data set: %d' % len(coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the relevant parts of OMNI are neglecting 1998 and the last entry of 2018, which I remove below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni = omni[hrs_yr:-1].reset_index(drop=True)\n",
    "dst = omni['Dst-index, nT']\n",
    "q10 = coeff['q10']\n",
    "t = coeff['time'].values\n",
    "t_year = 2000+t/(365.25) # federico's time axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's include previous values of the $q_1^0$ series in the input data, adding a column then to omni_lr (for now, ideally I wouldn't need to know previous values to make an accurate prediction). Let's also drop the Dst column (I revisit Dst prediction in the [sister notebook](forecasting_Dst_LSTM.ipynb) to this).\n",
    "\n",
    "Other columns that we'll drop for the moment are:\n",
    "- Kp-index\n",
    "- AL-index\n",
    "- AU-index\n",
    "- AE-index\n",
    "- ap-index\n",
    "\n",
    "since I'm primarily interested in how explicitly physical measurements can be related to predictions of the external field.\n",
    "\n",
    "Let's also do a simplest case forecasting, using only the following columns (at the moment):\n",
    "- $B_z$\n",
    "- SW proton density\n",
    "- SW plasma speed\n",
    "- SW plasma temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni = omni.drop(columns=['Dst-index, nT', 'Kp-index', 'AE-index, nT', 'AL-index, nT', 'AU-index, nT', 'ap-index, nT'])\n",
    "\n",
    "#omni = omni[['BZ, nT (GSM)', 'SW Plasma Temperature, K', 'SW Plasma Speed, km/s', 'SW Proton Density, N/cm^3']]\n",
    "omni['q10'] = q10.values\n",
    "\n",
    "input_dim = len(omni.columns)\n",
    "output_dim = 1\n",
    "ndat = len(omni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(t_year, q10)\n",
    "ax.set_ylabel('$q_1^0$, nT')\n",
    "ax.set_xlabel('year')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare q10 with Dst\n",
    "\n",
    "Here is a scatter plot of the first zonal coefficient $q_1^0$ compared to Dst, demonstrating their similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = np.corrcoef(dst,q10)[0,1]**2\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(dst,q10)\n",
    "plt.ylabel('$q_1^0$')\n",
    "plt.xlabel('Dst')\n",
    "plt.title('$r^2$ = %1.2f' % r2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sign convention differs between datasets, but they both record quite similar values, except during the largest storms when $q_1^0$ takes on larger values than Dst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Keras\n",
    "\n",
    "Having imported the data, let's design, train, and test the LSTM network for this prediction problem. The LSTM specifics are in the [aux_keras.py](aux_keras.py) module, where two basic architectures are elaborated. One is stateless, in which batches of fixed length are fed to the LSTM during training, while another is stateful, with the entire unbroken training series fed into the LSTM, thereby capturing the entire time history of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Prepare Data\n",
    "\n",
    "Let's separate the data into training and testing sets. Let's also normalize all of the data according to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "lahead = 6\n",
    "train_percent = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "data_in_train, data_out_train, data_in_test, data_out_test = datasplit(omni.values, \\\n",
    "                        q10.values.reshape(-1,1), batch_size, train_percent, lahead, stateful=True)\n",
    "\n",
    "# normalize\n",
    "scaler_input = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_in_train = scaler_input.fit_transform(data_in_train)\n",
    "data_in_test = scaler_input.transform(data_in_test)\n",
    "\n",
    "scaler_output = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_out_train = scaler_output.fit_transform(data_out_train)\n",
    "data_out_test = scaler_output.transform(data_out_test)\n",
    "\n",
    "# reshape input data\n",
    "data_in_train = data_in_train.reshape(-1, 1, input_dim)\n",
    "data_in_test = data_in_test.reshape(-1, 1, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn, hist = train_network(data_in_train, data_out_train, batch_size, epochs=100, stateful=True)\n",
    "plt.plot(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out_pred = test_network(data_in_test, data_out_test, rnn, batch_size,'$\\epsilon_1^0$', scaler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Torch\n",
    "\n",
    "Now, let's practice with Torch and develop a similar LSTM network for q10 forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to practice with the torch Dataset and Dataloader classes, which I'll do at some future point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form dataset and dataloader classes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I specify a model class following the tutorials at https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html and http://www.jessicayung.com/lstms-for-time-series-in-pytorch/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # we'll also need an output layer (equivalent of dense in keras)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda(),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda())\n",
    "\n",
    "    # number of input data must be divisible by batch_size\n",
    "    def forward(self, input):\n",
    "        self.hidden = self.init_hidden()\n",
    "        lstm_out, self.hidden = self.lstm(input.view(-1, self.batch_size, self.input_dim), self.hidden)\n",
    "        train_pred = self.linear(lstm_out)\n",
    "        return train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make input data into torch tensors with cuda support for running on the gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training data into cuda tensors\n",
    "data_in_train = torch.from_numpy(data_in_train).float().cuda()\n",
    "data_out_train = torch.from_numpy(data_out_train).float().cuda()\n",
    "\n",
    "# make testing data into cuda tensors\n",
    "data_in_test = torch.from_numpy(data_in_test).float().cuda()\n",
    "data_out_test = torch.from_numpy(data_out_test).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "batch_size = 500\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTM(input_dim, hidden_dim, batch_size, output_dim, num_layers)\n",
    "\n",
    "# run on gpu\n",
    "model.cuda()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "nepochs = 20000\n",
    "hist = np.zeros(nepochs)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    optimizer.zero_grad()\n",
    "    model.hidden = model.init_hidden()\n",
    "    train_pred = model(data_in_train)\n",
    "    loss = loss_function(train_pred, data_out_train.view(-1, batch_size, output_dim))\n",
    "    \n",
    "    hist[epoch] = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch %d, MSE: %1.2e' % (epoch, loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hidden = model.init_hidden()\n",
    "test_pred = model(data_in_test)\n",
    "test_pred = test_pred.reshape(-1,1)\n",
    "\n",
    "x = data_out_test.cpu().detach().numpy()\n",
    "y = test_pred.cpu().detach().numpy()\n",
    "plotcorr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, '.-')\n",
    "plt.plot(y, '.--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storm-time Prediction\n",
    "\n",
    "Let's repeat the above, but isolate training to storm times. We'll define geomagnetic storms as periods with more than 50 nT deviation (positively for $q_1^0$) from the baseline, which identifies the main phases of the storms. Given that the main phases are preceded by an initial phase of slightly increasing terrestrial external field (which for $q_1^0$ is a slight decrease), I'll include the 24 hours prior to the peak storm time. Furthermore, given that storms themselves can last for several days, I'll include the following 168 hours of data, which should in almost all cases cover the recovery phase. Since 168+24 is approximately 200, I'll just consider batches of 200 hours that encapsulate the geomagnetic storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, threshold all q10 values\n",
    "main_idx = q10 > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t_year, q10)\n",
    "plt.plot(t_year[main_idx], q10[main_idx], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to be careful about detecting multiples storms in close proximity, however, since these are likely to be the same storm, and so we should not double count it in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhr_before = 48\n",
    "nhr_after = 178\n",
    "\n",
    "# indices of q10 > 50\n",
    "thres_idx = findseq(main_idx, 1)\n",
    "\n",
    "# begin with first entry of thresholded indices\n",
    "storm_idx = thres_idx[0,:].reshape(1,-1)\n",
    "# counter\n",
    "c = 0\n",
    "for ii in range(len(thres_idx)-1):\n",
    "    if (thres_idx[ii+1,0] - storm_idx[c,1]) < 48:\n",
    "        storm_idx[c,1] = thres_idx[ii+1,1]\n",
    "        storm_idx[c,2] = storm_idx[c,1] - storm_idx[c,0] + 1\n",
    "    else:\n",
    "        storm_idx = np.append(storm_idx, thres_idx[ii+1,:].reshape(1,-1), axis=0)\n",
    "        c += 1\n",
    "\n",
    "nstorm = storm_idx.shape[0]\n",
    "batch_size = nhr_before + nhr_after\n",
    "\n",
    "for ii in range(nstorm):\n",
    "    storm_idx[ii,0] = np.max((0, storm_idx[ii,0]-nhr_before))\n",
    "    storm_idx[ii,1] = np.min((ndat, storm_idx[ii,0]+nhr_after+nhr_before))\n",
    "\n",
    "storm_idx = storm_idx[:, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having identified storms, now split up the storms into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to modify the indices to reflect the time shift!\n",
    "lahead = 6\n",
    "\n",
    "rnd_ord = np.random.permutation(nstorm)\n",
    "\n",
    "ntrain = int(train_percent*nstorm)\n",
    "ntest = nstorm - ntrain\n",
    "\n",
    "train_idx = storm_idx[rnd_ord[0:ntrain]]\n",
    "test_idx = storm_idx[rnd_ord[ntrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_train = np.zeros((ntrain, batch_size, input_dim))\n",
    "data_out_train = np.zeros((ntrain, batch_size))\n",
    "\n",
    "for ii in range(ntrain):\n",
    "    data_in_train[ii, :, :] = omni.loc[train_idx[ii,0]-lahead:train_idx[ii,1]-lahead-1].values\n",
    "    data_out_train[ii, :] = q10[train_idx[ii,0]:train_idx[ii,1]]\n",
    "    \n",
    "data_in_test = np.zeros((ntest, batch_size, input_dim))\n",
    "data_out_test = np.zeros((ntest, batch_size))\n",
    "\n",
    "for ii in range(ntest):\n",
    "    data_in_test[ii, :, :] = omni.loc[test_idx[ii,0]-lahead:test_idx[ii,1]-lahead-1].values\n",
    "    data_out_test[ii, :] = q10[test_idx[ii,0]:test_idx[ii,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare data for pytorch LSTM network, train, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training data into cuda tensors\n",
    "data_in_train = torch.from_numpy(data_in_train).float().cuda()\n",
    "data_out_train = torch.from_numpy(data_out_train).float().cuda()\n",
    "\n",
    "# make testing data into cuda tensors\n",
    "data_in_test = torch.from_numpy(data_in_test).float().cuda()\n",
    "data_out_test = torch.from_numpy(data_out_test).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "batch_size = 500\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTM(input_dim, hidden_dim, batch_size, output_dim, num_layers)\n",
    "\n",
    "# run on gpu\n",
    "model.cuda()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "nepochs = 20000\n",
    "hist = np.zeros(nepochs)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    optimizer.zero_grad()\n",
    "    model.hidden = model.init_hidden()\n",
    "    train_pred = model(data_in_train)\n",
    "    loss = loss_function(train_pred, data_out_train.view(-1, batch_size, output_dim))\n",
    "    \n",
    "    hist[epoch] = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch %d, MSE: %1.2e' % (epoch, loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hidden = model.init_hidden()\n",
    "test_pred = model(data_in_test)\n",
    "test_pred = test_pred.reshape(-1,1)\n",
    "\n",
    "x = data_out_test.cpu().detach().numpy()\n",
    "y = test_pred.cpu().detach().numpy()\n",
    "plotcorr(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
